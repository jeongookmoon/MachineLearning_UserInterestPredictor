{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split \n\n# Modelling Algorithms\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n#print(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 242,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#loading necessary data set\ntrain_ndata = pd.read_csv(\"../input/cmpe-188-f18-classification-challenge/train.csv\")\ntrain_udata = pd.read_csv(\"../input/moddata/users.csv\")\ntest_ndata = pd.read_csv(\"../input/modtest/modtest.csv\")\ntest_udata = pd.read_csv(\"../input/moddata/users.csv\")\n",
      "execution_count": 243,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c9a37c344f85532f9bf193ef597d69fcc6672bb9"
      },
      "cell_type": "code",
      "source": "# checking shapes of data sets\nprint('train_udata shape: ', train_udata.shape)\nprint(\"train_udata.keys(): {}\".format(train_udata.keys()))\nprint('train_ndata shape: ', train_ndata.shape)\nprint(\"train_ndata.keys(): {}\".format(train_ndata.keys()))\nprint('test_ndata shape: ', test_ndata.shape)\nprint(\"test_ndata.keys(): {}\".format(test_ndata.keys()))\nprint()\n# merging train data with user info\nmerged_Train = train_ndata.merge(train_udata, on='user')\nprint('merged_Train shape: ', merged_Train.shape)\nprint(\"merged_Train.keys(): {}\".format(merged_Train.keys()))\n\n# merging test data with user info\nmerged_Test = test_ndata.merge(test_udata, on='user')\nprint('merged_Test shape: ', merged_Test.shape)\nprint(\"merged_Test.keys(): {}\".format(merged_Test.keys()))",
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": "train_udata shape:  (38209, 7)\ntrain_udata.keys(): Index(['user', 'locale', 'birthyear', 'gender', 'joinedAt', 'location',\n       'timezone'],\n      dtype='object')\ntrain_ndata shape:  (12319, 5)\ntrain_ndata.keys(): Index(['user', 'event', 'invited', 'timestamp', 'interested'], dtype='object')\ntest_ndata shape:  (3079, 5)\ntest_ndata.keys(): Index(['unique_id', 'user', 'event', 'invited', 'timestamp'], dtype='object')\n\nmerged_Train shape:  (12319, 11)\nmerged_Train.keys(): Index(['user', 'event', 'invited', 'timestamp', 'interested', 'locale',\n       'birthyear', 'gender', 'joinedAt', 'location', 'timezone'],\n      dtype='object')\nmerged_Test shape:  (3079, 11)\nmerged_Test.keys(): Index(['unique_id', 'user', 'event', 'invited', 'timestamp', 'locale',\n       'birthyear', 'gender', 'joinedAt', 'location', 'timezone'],\n      dtype='object')\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7140aef3af59e16414a99e865b3a781e044125ae"
      },
      "cell_type": "code",
      "source": "#filling Nan values\nmerged_Train_data = merged_Train.fillna('none')\nmerged_Test_data = merged_Test.fillna('none')\n\n#referenced from https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html\n#values = {'locale' : 'unknown', 'gender' : 'unknown', 'location' : 'unknown', 'timestamp' : 'unknown' , 'birthyear' : 0, 'timezone' : 'unknown'}\n\n#referenced from data preparation class source\n#from sklearn.preprocessing import Imputer\n#imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n#imputed_DF = pd.DataFrame(merged_Train_data.fit_transform(df))\n#imputed_DF.columns = merged_Train_data.columns\n#imputed_DF.index = merged_Train_data.index\n#imputed_DF.describe()\n",
      "execution_count": 245,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "4c94493ae2b7b5a7bee6c89e6ec2874d7f2011e9"
      },
      "cell_type": "code",
      "source": "#converting column values that caused error (too big value, Nan, etc..)\n\n#for Train data sets\n# hot-encoding referenced from http://www.insightsbot.com/blog/McTKK/python-one-hot-encoding-with-scikit-learn\nfrom sklearn.preprocessing import OneHotEncoder\nlocale_encode = OneHotEncoder()\n#gender_encode = OneHotEncoder()\n#joinedAt_encode = OneHotEncoder()\n#location_encode = OneHotEncoder()\n#timestamp_encode = OneHotEncoder()\n#birthyear_encode = OneHotEncoder()\n\nencoded_locale = locale_encode.fit_transform(merged_Train_data.locale.values.reshape(-1,1)).toarray()\n#encoded_gender = gender_encode.fit_transform(merged_Train_data.gender.values.reshape(-1,1)).toarray()\n#encoded_joinedAt = joinedAt_encode.fit_transform(merged_Train_data.joinedAt.values.reshape(-1,1)).toarray()\n#encoded_location = location_encode.fit_transform(merged_Train_data.location.values.reshape(-1,1)).toarray()\n#encoded_timestamp = timestamp_encode.fit_transform(merged_Train_data.timestamp.values.reshape(-1,1)).toarray()\n#encoded_birthyear = birthyear_encode.fit_transform(merged_Train_data.birthyear.values.reshape(-1,1)).toarray()\n\ntestOneHot = pd.DataFrame(encoded_locale, columns = [\"locale\"+str(int(i)) for i in range(encoded_locale.shape[1])])\nmerged_Train_data = pd.concat([merged_Train_data, testOneHot], axis=1)\n\n#testOneHot = pd.DataFrame(encoded_gender, columns = [\"gender\"+str(int(i)) for i in range(encoded_gender.shape[1])])\n#merged_Train_data = pd.concat([merged_Train_data, testOneHot], axis=1)\n\n#testOneHot = pd.DataFrame(encoded_joinedAt, columns = [\"joinedAt\"+str(int(i)) for i in range(encoded_joinedAt.shape[1])])\n#merged_Train_data = pd.concat([merged_Train_data, testOneHot], axis=1)\n\n#testOneHot = pd.DataFrame(encoded_location, columns = [\"location\"+str(int(i)) for i in range(encoded_location.shape[1])])\n#merged_Train_data = pd.concat([merged_Train_data, testOneHot], axis=1)\n\n#testOneHot = pd.DataFrame(encoded_timestamp, columns = [\"timestamp\"+str(int(i)) for i in range(encoded_timestamp.shape[1])])\n#merged_Train_data = pd.concat([merged_Train_data, testOneHot], axis=1)\n\n#testOneHot = pd.DataFrame(encoded_birthyear, columns = [\"birthyear\"+str(int(i)) for i in range(encoded_birthyear.shape[1])])\n#merged_Train_data = pd.concat([merged_Train_data, testOneHot], axis=1)\n\n#for Test data sets\n# hot-encoding referenced from http://www.insightsbot.com/blog/McTKK/python-one-hot-encoding-with-scikit-learn\nlocale_encode_test = OneHotEncoder()\n#gender_encode_test = OneHotEncoder()\n#joinedAt_encode_test = OneHotEncoder()\n#location_encode_test = OneHotEncoder()\n#timestamp_encode_test = OneHotEncoder()\n#birthyear_encode_test = OneHotEncoder()\n\nencoded_localeT_test = locale_encode_test.fit_transform(merged_Test_data.locale.values.reshape(-1,1)).toarray()\n#encoded_genderT_test = gender_encode_test.fit_transform(merged_Test_data.gender.values.reshape(-1,1)).toarray()\n#encoded_joinedAt_test = joinedAt_encode_test.fit_transform(merged_Test_data.joinedAt.values.reshape(-1,1)).toarray()\n#encoded_location_test = location_encode_test.fit_transform(merged_Test_data.location.values.reshape(-1,1)).toarray()\n#encoded_timestamp_test = timestamp_encode_test.fit_transform(merged_Test_data.timestamp.values.reshape(-1,1)).toarray()\n#encoded_birthyear_test = birthyear_encode_test.fit_transform(merged_Test_data.birthyear.values.reshape(-1,1)).toarray()\n\ntestOneHot = pd.DataFrame(encoded_localeT_test, columns = [\"locale\"+str(int(i)) for i in range(encoded_localeT_test.shape[1])])\nmerged_Test_data = pd.concat([merged_Test_data, testOneHot], axis=1)\n\n#testOneHot = pd.DataFrame(encoded_genderT_test, columns = [\"gender\"+str(int(i)) for i in range(encoded_genderT_test.shape[1])])\n#merged_Test_data = pd.concat([merged_Test_data, testOneHot], axis=1)\n\n#testOneHot = pd.DataFrame(encoded_joinedAt_test, columns = [\"joinedAt\"+str(int(i)) for i in range(encoded_joinedAt_test.shape[1])])\n#merged_Test_data = pd.concat([merged_Test_data, testOneHot], axis=1)\n\n#testOneHot = pd.DataFrame(encoded_location_test, columns = [\"location\"+str(int(i)) for i in range(encoded_location_test.shape[1])])\n#merged_Test_data = pd.concat([merged_Test_data, testOneHot], axis=1)\n\n#testOneHot = pd.DataFrame(encoded_timestamp_test, columns = [\"timestamp\"+str(int(i)) for i in range(encoded_timestamp_test.shape[1])])\n#merged_Test_data = pd.concat([merged_Test_data, testOneHot], axis=1)\n\n#testOneHot = pd.DataFrame(encoded_birthyear_test, columns = [\"birthyear\"+str(int(i)) for i in range(encoded_birthyear_test.shape[1])])\n#merged_Test_data = pd.concat([merged_Test_data, testOneHot], axis=1)\n\n",
      "execution_count": 246,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee9b31d8aecbbab1f9abebc027d15ae9f4a74f69"
      },
      "cell_type": "code",
      "source": "# prepare training/test data sets for machine learning\n\n# drop features that contain values that cause errors\nX_Candidate = merged_Train_data.drop(['user','event','interested', 'locale', 'gender', 'joinedAt', 'location', 'timestamp', 'birthyear', 'timezone', 'joinedAt'], axis=1)\n#print('X_Candidate shape: ', X_Candidate.shape)\n#print(\"X_Candidate.keys(): {}\".format(X_Candidate.keys()))\n\n#select 'interested' column for target\ny_Candidate = merged_Train_data[['interested']]\n#print('y_Candidate shape: ', y_Candidate.shape)\n#print(\"y_Candidate.keys(): {}\".format(y_Candidate.keys()))\n\n# drop features that contain values that cause errors\nT_Candidate = merged_Test_data.drop(['unique_id','user','event','locale', 'gender', 'joinedAt', 'location', 'timestamp', 'birthyear', 'timezone', 'joinedAt'], axis=1)\n#print('T_Candidate shape: ', T_Candidate.shape)\n#print(\"T_Candidate.keys(): {}\".format(T_Candidate.keys()))\n\n# find common features between training/test data after hot one encode\n#print(X_Candidate.columns & T_Candidate.columns)\n\n# train data containiing various features\nX = X_Candidate[X_Candidate.columns & T_Candidate.columns]\nprint('X shape: ', X.shape)\nprint(\"X.keys(): {}\".format(X.keys()))\n\n# target data\ny = y_Candidate\nprint('y_Candidate shape: ', y_Candidate.shape)\nprint(\"y_Candidate.keys(): {}\".format(y_Candidate.keys()))\n\n#test data\nT = T_Candidate[X_Candidate.columns & T_Candidate.columns]\nprint('T shape: ', T.shape)\nprint(\"T.keys(): {}\".format(T.keys()))",
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": "X shape:  (12319, 16)\nX.keys(): Index(['invited', 'locale0', 'locale1', 'locale2', 'locale3', 'locale4',\n       'locale5', 'locale6', 'locale7', 'locale8', 'locale9', 'locale10',\n       'locale11', 'locale12', 'locale13', 'locale14'],\n      dtype='object')\ny_Candidate shape:  (12319, 1)\ny_Candidate.keys(): Index(['interested'], dtype='object')\nT shape:  (3079, 16)\nT.keys(): Index(['invited', 'locale0', 'locale1', 'locale2', 'locale3', 'locale4',\n       'locale5', 'locale6', 'locale7', 'locale8', 'locale9', 'locale10',\n       'locale11', 'locale12', 'locale13', 'locale14'],\n      dtype='object')\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "319e19aec9ad34718c80089144d8ba8cf70fb742"
      },
      "cell_type": "code",
      "source": "# splitting into train/test set\n#from sklearn.model_selection import train_test_split  \n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)  \n# train the machine to build model\n#from sklearn.tree import DecisionTreeClassifier  \n#classifier = DecisionTreeClassifier()  \n#classifier.fit(X, y)  \n\n# predict with test data X_test\n#y_pred = classifier.predict(X_test)\n#referencing https://stackabuse.com/decision-trees-in-python-with-scikit-learn/\n#from sklearn.metrics import classification_report, confusion_matrix  \n#print(\"Accuracy on training set: {:.3f}\".format(classifier.score(X_train, y_train)))\n#print(\"Accuracy on test set: {:.3f}\".format(classifier.score(X_test, y_test)))\n#print(confusion_matrix(y_test, y_pred))  \n#print(classification_report(y_test, y_pred))  \n",
      "execution_count": 248,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "abbfbdaace01560c899f1a63881b9985ecaedad7"
      },
      "cell_type": "code",
      "source": "#from sklearn.model_selection import train_test_split  \n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)  \n\n#from sklearn.ensemble import RandomForestClassifier\n#forest = RandomForestClassifier(n_estimators=100, random_state=0, max_depth=3) #try changing the number of features or max depth\n#forest.fit(X_train, y_train)\n\n#print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\n#print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))",
      "execution_count": 249,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cb3f45d96c38ba1598a0bf0ad772ca6a5bcb49f5"
      },
      "cell_type": "code",
      "source": "# reference from https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\nfrom sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X, y) \n\n#print(\"Accuracy on training set: {:.3f}\".format(neigh.score(X_train, y_train)))\n#print(\"Accuracy on test set: {:.3f}\".format(neigh.score(X_test, y_test)))\nprediction = neigh.predict(T)\n\n",
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  \"\"\"\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "ea56a3289a7635cd93fa1e0c79d5fb4c9ce7cf46"
      },
      "cell_type": "code",
      "source": "# train the machine to build model\n#from sklearn.tree import DecisionTreeClassifier  \n#classifier = DecisionTreeClassifier()  \n#classifier.fit(X, y)  \n# predict with test data T\n#prediction = classifier.predict(T)\n# check length of prediction data\n#print(len(prediction))",
      "execution_count": 251,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee82f61c3984988acd31a87aaa4ef17230c549ad"
      },
      "cell_type": "code",
      "source": "\n# train the machine to build model\n#from sklearn.ensemble import RandomForestClassifier\n#forest = RandomForestClassifier(n_estimators=100, random_state=0, max_depth=2)\n#forest.fit(X, y)\n# predict with test data T\n#prediction = forest.predict(T)\n# check length of prediction data\n#print(len(prediction))\n",
      "execution_count": 252,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0f90e28fbf1ad21358af20bb46e3672bd5856176"
      },
      "cell_type": "code",
      "source": "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)  \n\n#from sklearn import svm\n\n#svcModel = svm.SVC(kernel='poly')\n\n#svcModel.fit(X_train, y_train)\n\n#print(\"Accuracy on training set: {:.3f}\".format(gnbmodel.score(X_train, y_train)))\n#print(\"Accuracy on test set: {:.3f}\".format(gnbmodel.score(X_test, y_test)))\n\n#y_pred = clf.predict(X_test)",
      "execution_count": 253,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "61aca383f2a4774cbb618d104a092614e8dbde3c"
      },
      "cell_type": "code",
      "source": "#from sklearn.model_selection import train_test_split  \n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)  \n\n# referenced from https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/\n#Import Library of Gaussian Naive Bayes model\n#from sklearn.naive_bayes import GaussianNB\n#Create a Gaussian Classifier\n#gnbmodel = GaussianNB()\n\n# Train the model using the training sets \n#gnbmodel.fit(X_train, y_train)\n\n#print(\"Accuracy on training set: {:.3f}\".format(gnbmodel.score(X_train, y_train)))\n#print(\"Accuracy on test set: {:.3f}\".format(gnbmodel.score(X_test, y_test)))\n\n#Predict Output \n#predicted= model.predict()\n",
      "execution_count": 254,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6bb4de826f5b59022bb18aa5527e628cef735f95"
      },
      "cell_type": "code",
      "source": "# convert panda data fram into np array and switch num of colm to num of row\npred_shape_conv = prediction[:, np.newaxis]\npred_df = pd.DataFrame(pred_shape_conv.reshape(len(pred_shape_conv), 1), columns = ['interested'])\n#print(newTemp.columns)\n#print(newTemp.head())\n\n#select 'unique_id' only from test data\ntest_tf = merged_Test_data[['unique_id']]\n#print(result.columns)\n#print(result.head())\n\noutput = pd.concat([test_tf, pred_df], axis=1)\nprint(output.head())\nheader = [\"unique_id\", \"interested\"]\noutput.to_csv('resultFile.csv', columns = header)",
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "text": "   unique_id  interested\n0          1           0\n1          2           0\n2          3           0\n3          4           0\n4          5           0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "366b4fdd15008bd4fce99058eedbbb63afb5d2f0"
      },
      "cell_type": "code",
      "source": "#hotencoded some of columns since converting string to float was impossible for machine learning below\n#referneced from https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding\n#one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n#from sklearn.model_selection import cross_val_score\n#from sklearn.ensemble import RandomForestRegressor\n\n#def get_mae(X, y):\n    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n#    return -1 * cross_val_score(RandomForestRegressor(50), \n#                                X, y, \n#                                scoring = 'neg_mean_absolute_error').mean()\n\n#predictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\n\n#mae_without_categoricals = get_mae(predictors_without_categoricals, target)\n\n#mae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)\n\n#print('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\n#print('Mean Abslute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f0ff9f210cc33255c570237266a2bb25e1528b97"
      },
      "cell_type": "code",
      "source": "# for using only 1 column for classifier and test for prediction\n#testSet = test_ndata.drop(['user', 'event', 'invited', 'timestamp'], axis=1)\n#print(testSet.columns)\n\n#y_pred = classifier.predict(testSet)\n\n#print(len(y_pred))\n#y_pre = y_pred.astype(int)\n\n#newShape = y_pred[:, np.newaxis]\n#newTemp = pd.DataFrame(newShape.reshape(len(newShape), 1), columns = ['interested'])\n#print(newTemp.columns)\n\n#resultTest = pd.read_csv(\"../input/modtest/modtest.csv\")\n#result = resultTest.drop(['user', 'event', 'invited', 'timestamp'], axis=1)\n#result = pd.concat([result, newTemp], axis=1)\n#print(result.columns)\n#print(result.head())\n#header = [\"unique_id\", \"interested\"]\n#result.to_csv('resultFile.csv', columns = header)\n\n\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}